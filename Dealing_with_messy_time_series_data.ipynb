{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are classical effective time series models that deal with both seasonal and nonseasonal data, such as ARMA(autoregressiveâ€“moving-average) and ARIMA(autoregressive integrated moving average). In practice, I found it hard to apply these standard models, due to different objectives and more seriously, very fragmented and nonuniform times series data. \n",
    "\n",
    "This showed up in two recent Kaggle competitions, [Santander](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion) and [Home Credit](https://www.kaggle.com/c/home-credit-default-risk). For each customer, there are many time-stamped data such as transaction amount and payment and overdue information. The goal can be, for example, to predict the next nonzero transaction amount or the whether the customer will pay debt on time next time. However, data can be highly fragmented, i.e. data are only available for certain nonconsecutive past months. Also, data are highly nonuniform, i.e. the availability of data varies a lot across customers. This is either due to some customers having more transactions to begin with, or because there were issue getting data for certain customers. In both these cases, only relative time is given. The current time for different samples might be different but unknown.\n",
    "\n",
    "I came up with a very simple way to extract features out of such messy situations. They are not necessarily all useful, but machine learning models can automatically pick up the useful ones. What is important is that we are able to manufacture reasonably meaningful features very easily. These extra features allowed me to move up the Kaggle leaderboard by thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The starting point is the basic idea that more recent data in these cases should matter more, so we give more recent data higher exponential weights. To deal with the previously mentioned issues caused by fragmented and nonuniform data, we compute the weighted average of only the available data, and also treat the denominator and numerator as new features as well. The denominator is analogous to \"count\" in plain aggregation, and the numerator is analogous to \"sum\".\n",
    "\n",
    "The problem left now is how to choose the weighting. Due to the bad behavior of the data (many customers may have only a few nonzero data entries, as opposed to some others with hundreds of pieces of data history), it is not possible to run a typical time series model to train the weighting. Therefore we just handpick a set of decay constants and produce a set of features from each of them. In practice, I chose the following set:\n",
    "\n",
    "`decay_rates = [0.99, 0.95, 0.90, 0.82, 0.67]`\n",
    "\n",
    "We can compute their relavance range, i.e. how many months we need to go back so that the weight is below 0.1.\n",
    "\n",
    "* $\\frac{log(0.1)}{log(0.99)} \\approx 229$ (so this is almost like a plain aggregation, which is not that useful)\n",
    "\n",
    "* $\\frac{log(0.1)}{log(0.95)} \\approx 45$ (about 4 years back)\n",
    "\n",
    "* $\\frac{log(0.1)}{log(0.90)} \\approx 22$ (about 2 years back)\n",
    "\n",
    "* $\\frac{log(0.1)}{log(0.82)} \\approx 12$ (about 1 year back)\n",
    "\n",
    "* $\\frac{log(0.1)}{log(0.67)} \\approx 6$ (about 6 months back)\n",
    "\n",
    "Therefore they cover a wide range of relavant history lengths. They might be all useful to some extent, but, for example in decision trees, have different interactions with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we go to the implementation. We take advantage of pandas' great `groupby` method. Suppose our data contains the following columns:\n",
    "\n",
    "* `CustomerID`, the unique ID of the customer associated to the row. There can be many rows corresponding to an ID.\n",
    "\n",
    "* `Relative_Month`, the number of months back in time from the current time of that row.\n",
    "\n",
    "* `Data`, the relevant data of that row. There can be multiple other data columns. *Assume that irrelevant zero rows have already been removed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_info(df, rates, name=None):    \n",
    "    agg_dict=[]\n",
    "    for r in rates:\n",
    "        df['Weight_'+str(r)] = card['Relative_Month'].map(lambda x : r**(-x))\n",
    "        df['Data_'+str(r)] = card['Data'] * card['Data_'+str(r)]\n",
    "                \n",
    "        agg_dict.append(('Weight_'+str(r),'sum'))\n",
    "        agg_dict.append(('Data_'+str(r),'sum'))\n",
    "        # More items can be calculated, such as weighted max/min\n",
    "        \n",
    "    gp = card.groupby('CustomerID')    \n",
    "    result = pd.DataFrame(index = gp.groups.keys())\n",
    "    for key, func in agg_dict:\n",
    "        result[key+\"_\"+func] = gp[key].agg(func)\n",
    "    for r in rates:\n",
    "        result['Data_'+str(r)+'_Average'] = result['Data_'+str(r)+\n",
    "                                                   '_sum'] / result['Weight_'+str(r)+'_sum'] \n",
    "    \n",
    "    result.rename(columns=lambda x:name+\"_\"+x, inplace = True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rates = [0.99, 0.95, 0.90, 0.82, 0.67]\n",
    "df = pd.read_csv('chart1.csv')\n",
    "aggregated = compute_weighted_info(df, decay_rates, name = 'chart1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
