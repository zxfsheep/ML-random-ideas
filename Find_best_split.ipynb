{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One seemingly minute issue during cross validation is how to choose the random split. The worry is that the underlying distribution (of latent factors) of the training samples under the split might not be uniform. This is particularly prominent if the size of the dataset is small. For larger dataset, this might not have a big effect, but for more demanding situations such as contests, where 0.01% can make a difference, the impact is still visible.\n",
    "\n",
    "Of course, one can choose the lucky number instead of the default one as the random seed, or average over a group of random seeds. But for complicated models such as neural nets or gradient boosting trees, where each CV training can takes hours or longer, this is still costly and like a blind search. I found a simple trick to quickly choose a single or multiple decent splits.\n",
    "\n",
    "The intuition is that, even though baseline models as simple as linear regression do not perform as well as more complicated models, they usually capture a big chunk of nature of the samples. Therefore we use them like this:\n",
    "1. Try a large number of random splits on a simple model;\n",
    "\n",
    "2. For each split, we record the worst score among all the validation sets, i.e. across the different folds;\n",
    "\n",
    "3. At the end, we pick the random splits with the best records.\n",
    "\n",
    "So this is a min-max procedure. \n",
    "\n",
    "My experiments on different problems show that the results have the following nice properties:\n",
    "* Since the selected random splits have the best worst validation scores, the variation of their validation scores across folds tend to be small;\n",
    "\n",
    "* Even better, performing the same search procedure on different models, including complicated models such as gradient boosting trees, we get the same candidates most of the time. This indicates that these chosen random splits are, to some extent, universally good.\n",
    "\n",
    "Of course this cannot be guaranteed to work well for all problems. But at least it provides a simple heuristic for a quantitative selection of CV splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_split(esti, k, folds, rand_start, rand_end, params, X, y):\n",
    "    fold_rec = []\n",
    "    for i in range(rand_start, rand_end):\n",
    "        kf = KFold(n_splits=folds, random_state=i, shuffle=True)\n",
    "        gs = GridSearchCV(esti, params, scoring = 'neg_mean_squared_error', cv = kf, return_train_score = False, verbose = 2, refit= False)\n",
    "        gs.fit(X,y)\n",
    "        worst_split = min([gs.cv_results_[f'split{j}_test_score'][0] for j in range(folds)])\n",
    "        fold_rec.append((i, worst_split))\n",
    "    fold_rec.sort(key = lambda x: -x[1])\n",
    "    rands = [x[0] for x in fold_rec[:k]]\n",
    "    return rands, fold_rec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col = 0)\n",
    "cols = list(train.columns)\n",
    "cols.remove('TARGET')\n",
    "tar = 'TARGET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "gsparam = {}\n",
    "k = 20\n",
    "folds = 5\n",
    "rand_start = 0\n",
    "rand_end = 1000\n",
    "rand, all_rec = find_best_k_split(lr, k, folds, rand_start, rand_end, gsparam, train[cols], train[tar])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GPU]",
   "language": "python",
   "name": "conda-env-GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
